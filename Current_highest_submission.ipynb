{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "598EveY-LO6Z",
        "outputId": "edf8712e-6b5b-43f4-bddc-5b6498870ced"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's binary_logloss: 0.0417091\n",
            "[200]\tvalid's binary_logloss: 0.0257679\n",
            "[300]\tvalid's binary_logloss: 0.0225499\n",
            "[400]\tvalid's binary_logloss: 0.0209275\n",
            "[500]\tvalid's binary_logloss: 0.0200807\n",
            "[600]\tvalid's binary_logloss: 0.0196075\n",
            "[700]\tvalid's binary_logloss: 0.0192866\n",
            "[800]\tvalid's binary_logloss: 0.0189679\n",
            "[900]\tvalid's binary_logloss: 0.0187957\n",
            "[1000]\tvalid's binary_logloss: 0.0187086\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[1000]\tvalid's binary_logloss: 0.0187086\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's binary_logloss: 0.0406628\n",
            "[200]\tvalid's binary_logloss: 0.0251633\n",
            "[300]\tvalid's binary_logloss: 0.0224185\n",
            "[400]\tvalid's binary_logloss: 0.0209055\n",
            "[500]\tvalid's binary_logloss: 0.0202688\n",
            "[600]\tvalid's binary_logloss: 0.019621\n",
            "[700]\tvalid's binary_logloss: 0.0193418\n",
            "[800]\tvalid's binary_logloss: 0.0190651\n",
            "[900]\tvalid's binary_logloss: 0.0188436\n",
            "[1000]\tvalid's binary_logloss: 0.0188054\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[991]\tvalid's binary_logloss: 0.018804\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's binary_logloss: 0.041066\n",
            "[200]\tvalid's binary_logloss: 0.0258104\n",
            "[300]\tvalid's binary_logloss: 0.0232449\n",
            "[400]\tvalid's binary_logloss: 0.0218325\n",
            "[500]\tvalid's binary_logloss: 0.0209763\n",
            "[600]\tvalid's binary_logloss: 0.0205364\n",
            "[700]\tvalid's binary_logloss: 0.0202166\n",
            "[800]\tvalid's binary_logloss: 0.0199259\n",
            "[900]\tvalid's binary_logloss: 0.019812\n",
            "[1000]\tvalid's binary_logloss: 0.0197244\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[998]\tvalid's binary_logloss: 0.0197201\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's binary_logloss: 0.042476\n",
            "[200]\tvalid's binary_logloss: 0.0260128\n",
            "[300]\tvalid's binary_logloss: 0.0227953\n",
            "[400]\tvalid's binary_logloss: 0.0213948\n",
            "[500]\tvalid's binary_logloss: 0.0203616\n",
            "[600]\tvalid's binary_logloss: 0.0201892\n",
            "[700]\tvalid's binary_logloss: 0.0197749\n",
            "[800]\tvalid's binary_logloss: 0.0195196\n",
            "[900]\tvalid's binary_logloss: 0.0193071\n",
            "[1000]\tvalid's binary_logloss: 0.0191839\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[990]\tvalid's binary_logloss: 0.0191735\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid's binary_logloss: 0.039418\n",
            "[200]\tvalid's binary_logloss: 0.0257702\n",
            "[300]\tvalid's binary_logloss: 0.0224668\n",
            "[400]\tvalid's binary_logloss: 0.0207424\n",
            "[500]\tvalid's binary_logloss: 0.0200326\n",
            "[600]\tvalid's binary_logloss: 0.0195052\n",
            "[700]\tvalid's binary_logloss: 0.0190857\n",
            "[800]\tvalid's binary_logloss: 0.0187611\n",
            "[900]\tvalid's binary_logloss: 0.018596\n",
            "[1000]\tvalid's binary_logloss: 0.0184552\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[971]\tvalid's binary_logloss: 0.0184085\n",
            "Best Threshold from OOF: 0.44\n",
            "OOF F1 Score: 0.9720365831789108\n",
            "Classification Report (OOF):\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0       1.00      1.00      1.00    328404\n",
            "         1.0       0.98      0.96      0.97     42299\n",
            "\n",
            "    accuracy                           0.99    370703\n",
            "   macro avg       0.99      0.98      0.98    370703\n",
            "weighted avg       0.99      0.99      0.99    370703\n",
            "\n",
            "Confusion Matrix (OOF):\n",
            " [[327711    693]\n",
            " [  1646  40653]]\n",
            "Submission file created with tuned threshold from OOF predictions.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "import lightgbm as lgb\n",
        "\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    R = 6371.0  # Earth radius in km\n",
        "    dlat = np.radians(lat2 - lat1)\n",
        "    dlon = np.radians(lon2 - lon1)\n",
        "\n",
        "    a = (np.sin(dlat/2)**2) + np.cos(np.radians(lat1)) * np.cos(np.radians(lat2)) * (np.sin(dlon/2)**2)\n",
        "    c = 2 * np.arcsin(np.sqrt(a))\n",
        "    return R * c\n",
        "\n",
        "def preprocess_data(df):\n",
        "    # Convert date and time features\n",
        "    df['trans_date_dt'] = pd.to_datetime(df['trans_date'], errors='coerce')\n",
        "    df['trans_hour'] = pd.to_datetime(df['trans_time'], format='%H:%M:%S', errors='coerce').dt.hour\n",
        "    df['trans_day'] = df['trans_date_dt'].dt.day\n",
        "    df['trans_month'] = df['trans_date_dt'].dt.month\n",
        "    df['trans_year'] = df['trans_date_dt'].dt.year\n",
        "    df['day_of_week'] = df['trans_date_dt'].dt.dayofweek\n",
        "\n",
        "    # Distance feature (Euclidean approximation)\n",
        "    # For a more accurate measure, use haversine_distance:\n",
        "    #df['distance'] = haversine_distance(df['lat'], df['long'], df['merch_lat'], df['merch_long'])\n",
        "    df['distance'] = np.sqrt((df['lat'] - df['merch_lat'])**2 + (df['long'] - df['merch_long'])**2)\n",
        "\n",
        "    # Encode categorical variables\n",
        "    categorical_columns = ['category', 'gender', 'state', 'job']\n",
        "    for col in categorical_columns:\n",
        "        if col in df.columns:\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_aggregates(train_df, test_df):\n",
        "    agg = train_df.groupby('cc_num')['amt'].agg(['mean', 'median', 'std', 'count', 'max', 'min']).reset_index()\n",
        "    agg.columns = ['cc_num', 'cc_mean', 'cc_median', 'cc_std', 'cc_count', 'cc_max', 'cc_min']\n",
        "\n",
        "    # Merge aggregates back\n",
        "    train_df = train_df.merge(agg, on='cc_num', how='left')\n",
        "    test_df = test_df.merge(agg, on='cc_num', how='left')\n",
        "\n",
        "    # Ratio features\n",
        "    for df in [train_df, test_df]:\n",
        "        df['amt_to_mean'] = df['amt'] / (df['cc_mean'] + 1e-5)\n",
        "        df['amt_to_median'] = df['amt'] / (df['cc_median'] + 1e-5)\n",
        "        df['amt_to_max'] = df['amt'] / (df['cc_max'] + 1e-5)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def final_cleanup(df, is_train=True):\n",
        "    drop_columns = ['trans_num', 'trans_date', 'trans_time', 'first', 'last', 'dob', 'merchant',\n",
        "                    'street', 'city', 'state', 'zip', 'job', 'trans_date_dt', 'cc_num']\n",
        "    df.drop(columns=[c for c in drop_columns if c in df.columns], inplace=True, errors='ignore')\n",
        "\n",
        "    imputer = SimpleImputer(strategy='mean')\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df[numeric_cols] = imputer.fit_transform(df[numeric_cols])\n",
        "\n",
        "    if is_train:\n",
        "        y = df['is_fraud']\n",
        "        df.drop('is_fraud', axis=1, inplace=True)\n",
        "        return df, y\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('train.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "train_df = preprocess_data(train_df)\n",
        "test_df = preprocess_data(test_df)\n",
        "\n",
        "train_df, test_df = add_aggregates(train_df, test_df)\n",
        "X, y = final_cleanup(train_df, is_train=True)\n",
        "X_test = final_cleanup(test_df, is_train=False)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# LightGBM Cross-Validation\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros(len(X))\n",
        "models = []\n",
        "\n",
        "params = {\n",
        "    'objective': 'binary',\n",
        "    'metric': 'binary_logloss',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'learning_rate': 0.05,\n",
        "    'num_leaves': 31,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'seed': 42,\n",
        "    'verbose': -1\n",
        "}\n",
        "\n",
        "for fold, (trn_idx, val_idx) in enumerate(skf.split(X_scaled, y)):\n",
        "    X_train_fold, X_val_fold = X_scaled[trn_idx], X_scaled[val_idx]\n",
        "    y_train_fold, y_val_fold = y.iloc[trn_idx], y.iloc[val_idx]\n",
        "\n",
        "    dtrain = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
        "    dval = lgb.Dataset(X_val_fold, label=y_val_fold, reference=dtrain)\n",
        "\n",
        "    model = lgb.train(\n",
        "        params,\n",
        "        dtrain,\n",
        "        num_boost_round=1000,\n",
        "        valid_sets=[dval],\n",
        "        valid_names=['valid'],\n",
        "        callbacks=[\n",
        "            lgb.early_stopping(100),\n",
        "            lgb.log_evaluation(100)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    val_preds = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
        "    oof_preds[val_idx] = val_preds\n",
        "    models.append(model)\n",
        "\n",
        "# Find the best threshold using OOF predictions\n",
        "thresholds = np.linspace(0, 1, 101)\n",
        "best_f1 = 0\n",
        "best_thresh = 0.5\n",
        "for thr in thresholds:\n",
        "    y_oof_pred = (oof_preds > thr).astype(int)\n",
        "    score = f1_score(y, y_oof_pred)\n",
        "    if score > best_f1:\n",
        "        best_f1 = score\n",
        "        best_thresh = thr\n",
        "\n",
        "print(\"Best Threshold from OOF:\", best_thresh)\n",
        "print(\"OOF F1 Score:\", best_f1)\n",
        "\n",
        "# Evaluate on OOF data\n",
        "y_oof_pred = (oof_preds > best_thresh).astype(int)\n",
        "print(\"Classification Report (OOF):\\n\", classification_report(y, y_oof_pred))\n",
        "print(\"Confusion Matrix (OOF):\\n\", confusion_matrix(y, y_oof_pred))\n",
        "\n",
        "# Predict on test data by averaging predictions from all folds\n",
        "test_preds = np.zeros(len(X_test_scaled))\n",
        "for m in models:\n",
        "    test_preds += m.predict(X_test_scaled, num_iteration=m.best_iteration) / len(models)\n",
        "\n",
        "y_test_pred = (test_preds > best_thresh).astype(int)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({'id': test_df['id'], 'is_fraud': y_test_pred})\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "print(\"Submission file created with tuned threshold from OOF predictions.\")"
      ]
    }
  ]
}